# Herramientas de software para análisis exploratorio de datos

## Python - Jupyter
 Se especializa en juntar texto con código y es el favorita de los científicos de datos.

## AWS SAGEMAKER: 
Está diseñada para el deploy de los modelos de Machine Learning. También tiene notebooks y este se utiliza más para probar modelos con bastante poder de computo, porque cobra a partir del tiempo que se emplea. Otra ventaja que tiene es que podemos llevar todo lo que hagamos a deploy como modelo de producción, osea, a que se haga una API.

## AWS EMR: 
EMR es casi lo mismo que Apache Spark, ya que están diseñados para procesamiento de datos a grandes volúmenes tenido como base Hadoop. Es muy empleado para Data Engineer como preprocesamiento antes de un modelo o para modelos que requieren procesamiento en tiempo real. Si es que queremos que nuestro modelo sea en tiempo real, debemos de hablar con el Data Engineer para desde un inicio pedir este requerimiento.

## Google Jupyter Notebook Cloud: 
Es la forma de llevar nuestros notebooks a producción.

## Azure Notebooks: 
Azure apuesta que aquí solo se haga experimentación y que el deploy del modelo se haga en su plataforma.

## R Studio: 
Este entorno tiene el Kernel de R y es un software especializado en estadística. Tiene muchas más bibliotecas de estadística que en el caso de Python, apenas están llegando.

## Knime: 
Esta herramienta se enfoca para personas que no saben programar, es para personas que tiene conocimiento de negocio y no hay un experto que haga el flujo de trabajo del área de datos.

# Lecturas recomendadas

- [Jupyter](https://jupyter.org/)
- [GoogleColab](https://colab.research.google.com/)
- [Amazon SageMaker](https://aws.amazon.com/es/sagemaker/)
- [Apache Spark](https://spark.apache.org/)
- [Azure Notebooks](https://visualstudio.microsoft.com/es/vs/features/notebooks-at-microsoft/)
- [R studio](https://www.rstudio.com/)
- [KNIME](https://www.knime.com/)